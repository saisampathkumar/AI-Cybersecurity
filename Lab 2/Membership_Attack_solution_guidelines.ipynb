{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Membership Attack - solution guidelines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saisampathkumar/AI-Cybersecurity/blob/master/Lab%202/Membership_Attack_solution_guidelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvK6LaVhrfZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# required imports\n",
        "import sys \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O05dkTKAsEh-",
        "colab_type": "code",
        "outputId": "73cbab7b-4977-4a8c-eb14-f585e2a0e572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mount the google drive to download the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "project_path = '/content/drive/My Drive/cybersecurity'"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEW1mLverl7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create transforms to load the images, nothing much is needed here. \n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjsFJTFwr1wV",
        "colab_type": "code",
        "outputId": "f34aeee8-3977-4f7c-eaf5-6bd81867523a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# download CIFAR 10 training set\n",
        "trainset1 = torchvision.datasets.CIFAR10(root= project_path+'/data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "# load the trainning set\n",
        "trainloader1 = torch.utils.data.DataLoader(trainset1, batch_size=4, shuffle=True)\n",
        "\n",
        "# download the test data\n",
        "testset1 = torchvision.datasets.CIFAR10(root= project_path+'/data', train=False,\n",
        "                                        download=True, transform=transform_test)\n",
        "\n",
        "# load the test data\n",
        "testloader1 = torch.utils.data.DataLoader(testset1, batch_size=4, shuffle=True)\n",
        "\n",
        "classes1 = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTPw8Oymtlol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f9c0b60-63cd-45f1-b143-cc6cb6f86db3"
      },
      "source": [
        "# download CIFAR 10 training set\n",
        "trainset2 = torchvision.datasets.STL10(root= project_path+'/data', \n",
        "                                        split='train' ,download=True, transform=transform_train)\n",
        "\n",
        "# load the trainning set\n",
        "trainloader2 = torch.utils.data.DataLoader(trainset2, batch_size=4, shuffle=True)\n",
        "\n",
        "# download the test data\n",
        "testset2 = torchvision.datasets.STL10(root= project_path+'/data', \n",
        "                                        split='test',download=True, transform=transform_test)\n",
        "\n",
        "# load the test data\n",
        "testloader2 = torch.utils.data.DataLoader(testset2, batch_size=4, shuffle=True)\n",
        "\n",
        "classes2 = ('whale', 'shark', 'roses', 'cans', 'oranges', 'lamp', 'table', 'butterfly', 'lion', 'house')\n",
        "# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itY7G_y3tDnC",
        "colab_type": "code",
        "outputId": "4ef7722a-6611-4582-8739-a7b98235dcca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "# helper function to unnormalize and plot image \n",
        "def imshow(img):\n",
        "    img = np.array(img)\n",
        "    img = img / 2 + 0.5\n",
        "    img = np.moveaxis(img, 0, -1)\n",
        "    plt.imshow(img)\n",
        "    \n",
        "# display sample from dataset \n",
        "imgs, labels = iter(trainloader1).next()\n",
        "imshow(torchvision.utils.make_grid(imgs)) \n",
        "\n",
        "# notice who we converted the class idx to labels\n",
        "print(' '.join('%5s' % classes1[labels[j]] for j in range(4)))\n",
        "\n",
        "# run this cell multiple times and notice diff images\n",
        "\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " ship   dog  frog plane\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3dYWxJJvneO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "f933059e-203d-41bd-a76f-2f4b50b96957"
      },
      "source": [
        "# display sample from dataset \n",
        "imgs, labels = iter(trainloader2).next()\n",
        "imshow(torchvision.utils.make_grid(imgs)) \n",
        "\n",
        "# notice who we converted the class idx to labels\n",
        "print(' '.join('%5s' % classes2[labels[j]] for j in range(4)))\n",
        "\n",
        "# run this cell multiple times and notice diff images"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "roses whale  lamp shark\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQQZ0QRCEEuH/A6aWl16q82FzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJK32k42gbEK",
        "colab_type": "code",
        "outputId": "50bd6cf9-7e02-4642-88bb-df233f64a80a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# divide the training dataset into the required groups Make sure they are balanced\n",
        "# original trainset is made of 50k images\n",
        "\n",
        "total_size1 = len(trainset1)\n",
        "split1a = total_size1 // 4\n",
        "split2a = split1a * 2\n",
        "split3a = split1a * 3\n",
        "\n",
        "print(total_size1, split1a, split2a, split3a)\n",
        "\n",
        "indices1 = list(range(total_size1))\n",
        "\n",
        "total_size2 = len(trainset2)\n",
        "split1b = total_size2 // 4\n",
        "split2b = split1b * 2\n",
        "split3b = split1b * 3\n",
        "\n",
        "print(total_size2, split1b, split2b, split3b)\n",
        "\n",
        "indices2 = list(range(total_size2))\n",
        "\n",
        "# two groups to train the shadow (in and out)\n",
        "shadow_train_idx = indices1[:split1a]\n",
        "shadow_out_idx = indices1[split1a:split2a]\n",
        "\n",
        "# two groups to train the Target (in and out)\n",
        "target_train_idx = indices2[split2b:split3b]\n",
        "target_out_idx =  indices2[split3b:]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 12500 25000 37500\n",
            "5000 1250 2500 3750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4wJ_0lkhp76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16# pick your own\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset1, batch_size=batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx) # Pytorch function\n",
        "shadow_out_loader = torch.utils.data.DataLoader(trainset1, batch_size=batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx) # Pytorch function\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset2, batch_size=batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx) # Pytorch function\n",
        "target_out_loader = torch.utils.data.DataLoader(trainset2, batch_size=batch_size, sampler=target_out_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj9LuQJNuCXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# create a CNN\n",
        "# Input shape (3, 32, 32) \n",
        "# architecture: simple. 2 conv and 2 Max pool, followed by 2 fc (120, 84) \n",
        "# output of fc is 10 because we have 10 classes!\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"CNN.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        \n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O39H-xzClPgh",
        "colab_type": "code",
        "outputId": "ec0c789a-5ee7-4724-a11a-804f89374f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# check if CUDA available or not\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "# clear the cache\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER_B4V8YjKyU",
        "colab_type": "code",
        "outputId": "bf33ee08-b974-4e00-fe36-df393b47c978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initalize a target model and train it\n",
        "\n",
        "target_model = CNN()\n",
        "taget_model = target_model.cuda()\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss\n",
        "optimizer = optim.Adam(target_model.parameters(), lr=0.0003) # try Adam VS SGD\n",
        "\n",
        "    \n",
        "epochs = 20\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in target_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        logits = target_model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(\"\\nEpoch : {}/{}..\".format(e+1,epochs),f\"Training loss: {running_loss/len(target_train_loader)}\")\n",
        "\n",
        "#save the model\n",
        "print(\"Model: \\n\\n\", target_model, '\\n')\n",
        "torch.save(target_model.state_dict(), project_path+'/target_checkpoint.pth')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch : 1/20.. Training loss: 2.229173169860357\n",
            "\n",
            "Epoch : 2/20.. Training loss: 2.1305382840241056\n",
            "\n",
            "Epoch : 3/20.. Training loss: 2.106651988210557\n",
            "\n",
            "Epoch : 4/20.. Training loss: 2.0225892791265174\n",
            "\n",
            "Epoch : 5/20.. Training loss: 2.008793268022658\n",
            "\n",
            "Epoch : 6/20.. Training loss: 1.9845786079575745\n",
            "\n",
            "Epoch : 7/20.. Training loss: 1.972193462939202\n",
            "\n",
            "Epoch : 8/20.. Training loss: 1.9278805859481232\n",
            "\n",
            "Epoch : 9/20.. Training loss: 1.9439607707760003\n",
            "\n",
            "Epoch : 10/20.. Training loss: 1.9099413231958318\n",
            "\n",
            "Epoch : 11/20.. Training loss: 1.9222618296176572\n",
            "\n",
            "Epoch : 12/20.. Training loss: 1.9206467821628233\n",
            "\n",
            "Epoch : 13/20.. Training loss: 1.8822112083435059\n",
            "\n",
            "Epoch : 14/20.. Training loss: 1.8979822397232056\n",
            "\n",
            "Epoch : 15/20.. Training loss: 1.9032473473609248\n",
            "\n",
            "Epoch : 16/20.. Training loss: 1.9174515959582752\n",
            "\n",
            "Epoch : 17/20.. Training loss: 1.8486856493768813\n",
            "\n",
            "Epoch : 18/20.. Training loss: 1.8736573035203958\n",
            "\n",
            "Epoch : 19/20.. Training loss: 1.859326011017908\n",
            "\n",
            "Epoch : 20/20.. Training loss: 1.865379295771635\n",
            "Model: \n",
            "\n",
            " CNN(\n",
            "  (conv_layer): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU(inplace)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Dropout2d(p=0.05)\n",
            "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): ReLU(inplace)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_layer): Sequential(\n",
            "    (0): Dropout(p=0.1)\n",
            "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): ReLU(inplace)\n",
            "    (5): Dropout(p=0.1)\n",
            "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (7): LogSoftmax()\n",
            "  )\n",
            ") \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpY8ktdskRQN",
        "colab_type": "code",
        "outputId": "2bf0f88a-ae6c-42d6-eccd-2edb9962788a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate the accuracy of the Target Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in target_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = target_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 12500 test images (Cifar10 data): %d %%' % (100 * correct / total))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 12500 test images (Cifar10 data): 24 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEClKFqikmUl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha_VqRVVkoCm",
        "colab_type": "code",
        "outputId": "deb23a6c-957e-4ee3-917f-1389a81c3c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initalize a Shadow Model and Train it\n",
        "# for the first ICP, your shadow model can have the same CNN architecture and hyperparameters\n",
        "\n",
        "shadow_model = CNN()\n",
        "# clear the cache\n",
        "torch.cuda.empty_cache()\n",
        "#send to GPU\n",
        "shadow_model = shadow_model.cuda()\n",
        "shadow_criterion =  nn.CrossEntropyLoss() # CrossEntropyLoss\n",
        "shadow_optimizer = optim.Adam(shadow_model.parameters(), lr=0.0003) # ADAM \n",
        "\n",
        "\n",
        "# let the magic begin\n",
        "epochs = 20\n",
        "with torch.set_grad_enabled(True):\n",
        "  for e in range(epochs):\n",
        "      running_loss = 0\n",
        "      for images, labels in shadow_train_loader:\n",
        "          # sending tensors to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "          shadow_optimizer.zero_grad()\n",
        "          logits = shadow_model(images)\n",
        "          shadow_loss = shadow_criterion(logits, labels)\n",
        "          shadow_loss.backward()\n",
        "          shadow_optimizer.step()\n",
        "\n",
        "\n",
        "          running_loss += shadow_loss.item()\n",
        "      else:\n",
        "          print(\"\\nEpoch : {}/{}..\".format(e+1,epochs),f\"Training loss: {running_loss/len(shadow_train_loader)}\")\n",
        "\n",
        "#save the model\n",
        "print(\"Our model: \\n\\n\", shadow_model, '\\n')\n",
        "torch.save(shadow_model.state_dict(), project_path+'/shadow_checkpoint.pth')\n",
        "print('Finished Training the Shadow model')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch : 1/20.. Training loss: 1.7904457638940543\n",
            "\n",
            "Epoch : 2/20.. Training loss: 1.4927572009661008\n",
            "\n",
            "Epoch : 3/20.. Training loss: 1.291610455269094\n",
            "\n",
            "Epoch : 4/20.. Training loss: 1.1476264825028837\n",
            "\n",
            "Epoch : 5/20.. Training loss: 1.0468432715786693\n",
            "\n",
            "Epoch : 6/20.. Training loss: 0.9706698018495384\n",
            "\n",
            "Epoch : 7/20.. Training loss: 0.8879454076061468\n",
            "\n",
            "Epoch : 8/20.. Training loss: 0.8272526573075358\n",
            "\n",
            "Epoch : 9/20.. Training loss: 0.7750277130500131\n",
            "\n",
            "Epoch : 10/20.. Training loss: 0.7319775208297288\n",
            "\n",
            "Epoch : 11/20.. Training loss: 0.6785643851703695\n",
            "\n",
            "Epoch : 12/20.. Training loss: 0.6546884928841877\n",
            "\n",
            "Epoch : 13/20.. Training loss: 0.626808594502604\n",
            "\n",
            "Epoch : 14/20.. Training loss: 0.6039349687907397\n",
            "\n",
            "Epoch : 15/20.. Training loss: 0.5680619002512807\n",
            "\n",
            "Epoch : 16/20.. Training loss: 0.5446321343290893\n",
            "\n",
            "Epoch : 17/20.. Training loss: 0.5069927369218196\n",
            "\n",
            "Epoch : 18/20.. Training loss: 0.49844621229783426\n",
            "\n",
            "Epoch : 19/20.. Training loss: 0.4710648829912972\n",
            "\n",
            "Epoch : 20/20.. Training loss: 0.44965174294474636\n",
            "Our model: \n",
            "\n",
            " CNN(\n",
            "  (conv_layer): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU(inplace)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Dropout2d(p=0.05)\n",
            "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): ReLU(inplace)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_layer): Sequential(\n",
            "    (0): Dropout(p=0.1)\n",
            "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): ReLU(inplace)\n",
            "    (5): Dropout(p=0.1)\n",
            "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (7): LogSoftmax()\n",
            "  )\n",
            ") \n",
            "\n",
            "Finished Training the Shadow model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0kP_-62ljFE",
        "colab_type": "code",
        "outputId": "88dea4bf-4387-4776-c2e2-e8526a428366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate the accuracy of the Shadow Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = shadow_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        #print(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 1250 test images (STL10 data): %d %%' % (100 * correct / total))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 1250 test images (STL10 data): 76 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ymKnj7QpdDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = CNN()\n",
        "    model.load_state_dict(checkpoint)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J1B1OhMp6er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 1# pick your own\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset1, batch_size=batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx) # Pytorch function\n",
        "shadow_out_loader = torch.utils.data.DataLoader(trainset1, batch_size=batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx) # Pytorch function\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset2, batch_size=batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx) # Pytorch function\n",
        "target_out_loader = torch.utils.data.DataLoader(trainset2, batch_size=batch_size, sampler=target_out_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaD8_N34l3Eh",
        "colab_type": "code",
        "outputId": "11a71822-7389-4f1c-8344-5c525e6b01ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#load the model\n",
        "load_shadow_model = load_checkpoint(project_path+'/shadow_checkpoint.pth')\n",
        "load_shadow_model = load_shadow_model.cuda()\n",
        "\n",
        "# freeze the Shadow model \n",
        "for param in load_shadow_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# make predictions on both datasets (shadow_in and shdow_out)\n",
        "predictions = []\n",
        "\n",
        "labels_0 = 0#np.zeros(1)\n",
        "labels_1 = 1#np.ones(1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_shadow_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_1])   \n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_shadow_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_0]) \n",
        "        \n",
        "print(predictions[0])  \n",
        "print(predictions[17000])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([0.08596601, 0.04023437, 0.02021134, 0.01794253, 0.08859356,\n",
            "       0.03292565, 0.00500259, 0.5770346 , 0.00919348, 0.12289599],\n",
            "      dtype=float32), 1]\n",
            "[array([7.39586577e-02, 4.00119927e-04, 8.33559275e-01, 5.76939760e-03,\n",
            "       7.30520952e-03, 1.05175911e-03, 7.63668939e-02, 1.20160425e-04,\n",
            "       1.07317849e-03, 3.95484618e-04], dtype=float32), 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhaKjRRUl6Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a new dataset of the shape [predictions(shadow_in), 1], [predicitons(shadow_out), 0] and zip them together\n",
        "#save the dataset\n",
        "import pickle\n",
        "\n",
        "with open(project_path+'/data/shadow.data', 'wb') as filehandle:\n",
        "    pickle.dump(predictions, filehandle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl56BLIpl8Nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the recall and precision of your attack network using the Target_out and Target_in datasets\n",
        "# to do so, take a random numer of datapoints, run them throw the target model,\n",
        "#load the model\n",
        "load_target_model = load_checkpoint(project_path+'/target_checkpoint.pth')\n",
        "load_target_model = load_target_model.cuda()\n",
        "\n",
        "# freeze the Shadow model \n",
        "for param in load_target_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# make predictions on both datasets (shadow_in and shdow_out)\n",
        "predictions = []\n",
        "label_size = (1,1)\n",
        "\n",
        "labels_0 = 0\n",
        "labels_1 = 1\n",
        "with torch.no_grad():\n",
        "    for images, labels in target_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_target_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_1])   \n",
        "with torch.no_grad():\n",
        "    for images, labels in target_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_target_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_0]) \n",
        "        \n",
        "#save the dataset\n",
        "import pickle\n",
        "\n",
        "with open(project_path+'/data/target.data', 'wb') as filehandle:\n",
        "    pickle.dump(predictions, filehandle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yEnRcZW6sUY",
        "colab_type": "code",
        "outputId": "b7d2ee1c-7db0-4517-b9f7-7f2773fbfe55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# check if CUDA available or not\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "# clear the cache\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UZ2tfJzl94s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the dataset\n",
        "with open(project_path+'/data/shadow.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    predictionsList = pickle.load(filehandle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJFbXp8umAam",
        "colab_type": "code",
        "outputId": "cc4740cb-b798-427c-adbf-df2e43bc8d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total_size = len(predictionsList)\n",
        "split1 = total_size // 4\n",
        "split1 = total_size - split1 \n",
        "split2 = split1*2\n",
        "indices = list(range(total_size))\n",
        "\n",
        "# two groups to train the shadow (in and out)\n",
        "train_idx = indices[:] \n",
        "test_idx = indices[split1:] \n",
        "print(f'No.of train date {len(train_idx)} and No.of test data {len(test_idx)}')\n",
        "batch_size = 10 # pick your own\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "train_sampler = SubsetRandomSampler(train_idx) # Pytorch function\n",
        "train_loader = torch.utils.data.DataLoader(predictionsList, batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "test_sampler = SubsetRandomSampler(test_idx) # Pytorch function\n",
        "test_loader = torch.utils.data.DataLoader(predictionsList, batch_size=batch_size, sampler=test_sampler)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No.of train date 25000 and No.of test data 6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU012hhrmDvi",
        "colab_type": "code",
        "outputId": "4311a087-206b-499e-9cc3-964dd29a62dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# create the Attack Model: A NN binary classifier {0, 1}\n",
        "# the input to this model is the propability distribution vector of size 10\n",
        "# and the output is either 0 (input was not included in training) or 1\n",
        "from torch.autograd import Variable\n",
        "attack_model = nn.Sequential(nn.Linear(10, 20),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(20, 26),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(26, 16),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(16, 8),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(8, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "attack_model = attack_model.cuda()\n",
        "attack_criterion = nn.CrossEntropyLoss()\n",
        "attack_optimizer = optim.Adam(attack_model.parameters(), lr=0.003)\n",
        "\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for outputs, labels in train_loader:\n",
        "        # sending tensors to GPU\n",
        "        outputs = outputs.cuda().float()\n",
        "        labels = labels.cuda().long()\n",
        "        attack_optimizer.zero_grad()\n",
        "        pred = torch.exp(attack_model(outputs))\n",
        "        #print(pred.data, labels)\n",
        "        loss = attack_criterion(pred, labels)\n",
        "        loss.backward()\n",
        "        attack_optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(predictionsList)}\")\n",
        "        \n",
        "torch.save(attack_model.state_dict(), project_path+'/attack_checkpoint.pth')\n",
        "print('Finished Training the Attack model')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0.06935383262634277\n",
            "Training loss: 0.06934928565263748\n",
            "Training loss: 0.06932225021362305\n",
            "Training loss: 0.06931861157894134\n",
            "Training loss: 0.06930207609653473\n",
            "Finished Training the Attack model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWR_CXQEmIC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the dataset\n",
        "with open(project_path+'/data/target.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    validation = pickle.load(filehandle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecc2gaFmHnoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 1 # pick your own\n",
        "\n",
        "validation_sampler = SubsetRandomSampler(indices[500:2500]) # randomly picking 5000 data items\n",
        "validation_loader = torch.utils.data.DataLoader(validation, batch_size=batch_size, sampler=validation_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haNVIg-FHsFv",
        "colab_type": "code",
        "outputId": "71735a01-e338-4025-bc3a-e310703ed1d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#input the output of the target model to your attack network \n",
        "# you already know the target_in and target_out samples, so use that info to evaluate the attack model\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "total = 0\n",
        "tp = 0\n",
        "tn = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "with torch.no_grad():\n",
        "    for outputs, labels in validation_loader:\n",
        "        # sending tensors to GPU\n",
        "        outputs = outputs.cuda().float()\n",
        "        labels = labels.cuda().long()\n",
        "        pred = torch.exp(attack_model(outputs))\n",
        "        predicted = torch.argmax(pred.data)\n",
        "        #print('\\n',pred.data, predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.item() == labels.item())\n",
        "        tp += ((predicted.item() == labels.item()) and (predicted.item() == 1))\n",
        "        tn += ((predicted.item() == labels.item()) and (predicted.item() == 0))\n",
        "        fp += ((predicted.item() != labels.item()) and (predicted.item() == 1))\n",
        "        fn += ((predicted.item() != labels.item()) and (predicted.item() == 0))\n",
        "incorrect = total- correct\n",
        "print(f'TP : {tp}, TN : {tn}, FP : {fp}, FN : {fn}')\n",
        "pre = tp/(tp+fp)\n",
        "rec = tp/(tp+fn)\n",
        "print(f'Precision {pre*100}')\n",
        "print(f'Recall {rec*100}')\n",
        "print(f'F1 Score {2*((pre*rec)/(pre+rec))*100}')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP : 750, TN : 0, FP : 1250, FN : 0\n",
            "Precision 37.5\n",
            "Recall 100.0\n",
            "F1 Score 54.54545454545454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMsyCgziqeaa",
        "colab_type": "text"
      },
      "source": [
        "Great! At this point, you must have created a succesfful attack model that can detect whether a datapoint was used in training a target mode or not. \n",
        "* A successful attack model is one with a precision/recall higher than 85% -- you are using same architecture and are aware of the data classes\n",
        "\n",
        " \n",
        " Can you suggest any defense mechanism? If yes, Apply them to your solution and re-evaluate your attack model. How did your defense mecanism affect the accuracy of the target model? How did it affect the recall and precision of the Attack model?"
      ]
    }
  ]
}